# NLP Preâ€‘Processing Work

Bu Ã§alÄ±ÅŸma, 2023 yazÄ±nda gerÃ§ekleÅŸtirdiÄŸim bir staj sÃ¼recinde yaptÄ±ÄŸÄ±m **baÅŸlangÄ±Ã§ seviyesi bir doÄŸal dil iÅŸleme (NLP) Ã¶n iÅŸleme projesidir**. AmaÃ§, metin verileri Ã¼zerinde temel Ã¶n iÅŸleme adÄ±mlarÄ±nÄ± uygulayarak bu verileri analiz ve modelleme iÃ§in hazÄ±r hÃ¢le getirmektir.

---

## Ä°Ã§indekiler

- [Genel BakÄ±ÅŸ](#genel-bakÄ±ÅŸ)  
- [KullanÄ±lan Teknolojiler](#kullanÄ±lan-teknolojiler)  
- [Ã–n Ä°ÅŸleme AdÄ±mlarÄ±](#Ã¶n-iÌ‡ÅŸleme-adÄ±mlarÄ±)  
  - Temizleme  
  - Tokenizasyon  
  - Stop-word Ã§Ä±karÄ±mÄ±  
  - Stemming ve Lemmatization  
- [NasÄ±l KullanÄ±lÄ±r?](#nasÄ±l-kullanÄ±lÄ±r)

---

## Genel BakÄ±ÅŸ

Bu proje, metin verilerini analiz ve modelleme adÄ±mlarÄ±na hazÄ±rlamak iÃ§in gereken yaygÄ±n NLP Ã¶n iÅŸlem tekniklerini iÃ§erir. Hedeflenen temel iÅŸlemler:

- Ham metinleri temizlemek (HTML, URL, rakam, Ã¶zel karakter vb.)
- Metni tokenize edip anlamlÄ± parÃ§alara bÃ¶lmek
- Gereksiz kelimeleri Ã§Ä±karmak (stopâ€‘words)
- Kelimelerin temeldeki hÃ¢line dÃ¶nÃ¼ÅŸtÃ¼rÃ¼lmesi (stemming/lemmatization)

Bu adÄ±mlar sayesinde sentiment analizi, metin sÄ±nÄ±flandÄ±rma veya konu modelleme gibi NLP gÃ¶revleri iÃ§in veri hazÄ±rlÄ±ÄŸÄ± saÄŸlanÄ±r.

---

## KullanÄ±lan Teknolojiler

| Teknoloji            | KullanÄ±m AlanÄ±                      |
|----------------------|-------------------------------------|
| Python               | Temel programlama dili              |
| regex                | Metin temizleme (Ã¶rneÄŸin URLâ€™ler)   |
| NLTK / SpaCy         | Tokenizasyon, stopâ€‘word, lemmatizer |
| BeautifulSoup        | HTML etiket temizleme               |
| Gensim, Scikitâ€‘Learn | VektÃ¶r dÃ¶nÃ¼ÅŸÃ¼mleri (Ã¶rneÄŸin TFâ€‘IDF) |

---

## Ã–n Ä°ÅŸleme AdÄ±mlarÄ±

### ğŸ§¼ 1. Temizleme (Cleaning)

- KÃ¼Ã§Ã¼k harfe dÃ¶nÃ¼ÅŸtÃ¼rme (lowercase)
- HTML/XML etiketlerinin kaldÄ±rÄ±lmasÄ±
- URL, eâ€‘posta ve sayÄ±lar gibi Ã¶zel desenlerin silinmesi
- Noktalama iÅŸaretleri ve gereksiz karakterlerin temizlenmesi

### âœ‚ï¸ 2. Tokenizasyon

- CÃ¼mle ve kelime bazÄ±nda bÃ¶lme (Ã¶rneÄŸin `nltk.tokenize.word_tokenize`)
- GerektiÄŸinde Ã¶zel tokenizasyon (Ã¶rneÄŸin tweet, chat dili)

### ğŸš« 3. Stopâ€‘Word Ã‡Ä±karÄ±mÄ±

- SÄ±k kullanÄ±lan anlamsÄ±z kelimelerin (Ã¶rneÄŸin â€œveâ€, â€œtheâ€, â€œisâ€) metinden Ã§Ä±karÄ±lmasÄ±

### ğŸŒ± 4. KÃ¶k ve Lemma Ä°ÅŸlemleri

- **Stemming**: Kelimeleri kÃ¶k hÃ¢line (stem) indirgeme
- **Lemmatization**: BaÄŸlama gÃ¶re anlamlÄ± hÃ¢le lemmatize etme

---

## NasÄ±l KullanÄ±lÄ±r?

1. Bu depoyu klonlayÄ±n:

   ```bash
   git clone https://github.com/sefiye-arican/NLP-pre-processing-work.git
   cd NLP-pre-processing-work

2. Gerekli kÃ¼tÃ¼phaneleri kurun:
   ```bash
    pip install -r requirements.txt
   
3. Python ortamÄ±nda Ã¶rnek notebook'larÄ± aÃ§arak takip edin:
    ```bash
    jupyter notebook

- `text_cleaning.ipynb` (veya diÄŸer notebook dosyalarÄ±) Ã¼zerinde anlatÄ±mlÄ± Ã¶rnekler yer almaktadÄ±r.

4. AdÄ±m adÄ±m uygulama:

  - `clean_text()` fonksiyonu ile temizlik  
  - `tokenize_text()` ile tokenizasyon  
  - `remove_stopwords()` ile stopâ€‘word Ã§Ä±karÄ±mÄ±  
  - `stem_or_lemmatize()` ile kelime kÃ¶kÃ¼ne ya da lemmaâ€™ya dÃ¶nÃ¼ÅŸtÃ¼rme


# NLP Preâ€‘Processing Work

This project is a **beginner-level natural language processing (NLP) pre-processing study** I developed during my internship in the summer of 2023. The goal is to apply fundamental pre-processing steps on text data to prepare it for analysis and modeling.

---

## Table of Contents

- [Overview](#overview)  
- [Technologies Used](#technologies-used)  
- [Pre-processing Steps](#pre-processing-steps)  
  - Cleaning  
  - Tokenization  
  - Stop-word Removal  
  - Stemming and Lemmatization  
- [How to Use](#how-to-use)

---

## Overview

This project includes common NLP pre-processing techniques to prepare text data for analysis and modeling. The main operations are:

- Cleaning raw text (HTML tags, URLs, numbers, special characters, etc.)
- Tokenizing text into meaningful components
- Removing unnecessary words (stop-words)
- Reducing words to their base form (stemming/lemmatization)

These steps help prepare datasets for tasks such as sentiment analysis, text classification, and topic modeling.

---

## Technologies Used

| Technology           | Purpose                            |
|----------------------|-------------------------------------|
| Python               | Main programming language           |
| regex                | Pattern-based text cleaning         |
| NLTK / SpaCy         | Tokenization, stop-word removal, lemmatization |
| BeautifulSoup        | HTML tag removal                    |
| Gensim, Scikit-Learn | Text vectorization (e.g. TF-IDF)    |

---

## Pre-processing Steps

### ğŸ§¼ 1. Cleaning

- Convert text to lowercase
- Remove HTML/XML tags
- Remove patterns like URLs, emails, numbers
- Remove punctuation and unnecessary characters

### âœ‚ï¸ 2. Tokenization

- Split text into sentences and words (e.g., `nltk.tokenize.word_tokenize`)
- Custom tokenization for tweets or chat-style texts if needed

### ğŸš« 3. Stop-word Removal

- Remove common words that add little meaning (e.g., â€œandâ€, â€œtheâ€, â€œisâ€)

### ğŸŒ± 4. Stemming and Lemmatization

- **Stemming**: Reduce words to their stem forms
- **Lemmatization**: Reduce words to their lemma based on context

---

## How to Use

1. Clone this repository:

   ```bash
   git clone https://github.com/sefiye-arican/NLP-pre-processing-work.git
   cd NLP-pre-processing-work
